from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.hooks.base_hook import BaseHook

dag = DAG(dag_id='etl_update_user_data',  
    default_args={'owner': 'airflow',
                  'end_date' : datetime(2022, 6, 5),  
                  'email': ['airflow@example.com'],
                  'email_on_failure': False,
                  'email_on_retry': False,
                  'retries': 1,
                  'retry_delay': timedelta(minutes=5),
                  'start_date': datetime(2022, 5, 5)
                  },
    description = 'DAG simple',
    schedule_interval= "0 12 * * *",
    catchup= True,
    params = {'business_dt': '{{ ds }}'}
        )

def create_files_request():
    requests.post('some_url', data={'date':business_dt})
    time.sleep(5) # max time to generate file

# move files from s3 to local 
def get_files_from_s3(business_dt,s3_conn_name):
    BUCKET_NAME = 'my-bucket'
    s3 = boto3.resource(s3_conn_name)
    
    for s3_filename in ['customer_research.csv','user_order_log.csv',
                    'user_activity_log.csv','price_log.csv']:
        local_filaname = business_dt.replace('-','') + '_' + s3_filename # add date to filename
        
        s3.Bucket(BUCKET_NAME).download_file(s3_filename, local_filaname)

def load_file_to_pg(filename,pg_table,conn_args):
    ''' csv-files to pandas dataframe '''
    f = pd.read_csv(filename)
    
    ''' load data to postgres '''
    cols = list(f.columns)
    insert_stmt = f"INSERT INTO {pg_table} ({cols}) VALUES %s"
    
    pg_conn = psycopg2.connect(**conn_args)
    cur = pg_conn.cursor()
    psycopg2.extras.execute_values(cur, insert_stmt, f.values)
    pg_conn.commit()
    cur.close()
    pg_conn.close()
    
def pg_execute_query(query,conn_args):
    pg_conn = psycopg2.connect(**conn_args)
    cur = pg_conn.cursor()
    cur.execute(query)
    pg_conn.commit()
    cur.close()
    pg_conn.close()

create_files_request = PythonOperator(task_id='create_files_request',
                                    python_callable=create_files_request,
                                    #op_kwargs={'business_dt': business_dt},
                                    dag=dag)

get_files = PythonOperator(task_id='get_files_task',
												python_callable=get_files_from_s3,
                        #op_kwargs={'business_dt': business_dt,
						#											's3_conn_name': s3_conn_name},
                        dag=dag)

load_customer_research = PythonOperator(task_id='load_customer_research',
                                    python_callable=load_file_to_pg,
                                    #op_kwargs={'filename': business_dt.replace('-','') + '_customer_research.csv',
                                    #            'pg_table': 'staging.customer_research',
                                    #            'conn_args': pg_conn},
                                    dag=dag)

load_user_order_log = PythonOperator(task_id='load_user_order_log',
                                    python_callable=load_file_to_pg,
                                    #op_kwargs={'filename': business_dt.replace('-','') + '_user_order_log.csv',
                                     #           'pg_table': 'staging.user_order_log',
                                     #           'conn_args': pg_conn},
                                    dag=dag)

load_user_activity_log = PythonOperator(task_id='load_user_activity_log',
                                    python_callable=load_file_to_pg,
                                    #op_kwargs={'filename': business_dt.replace('-','') + '_user_activity_log.csv',
                                     #           'pg_table': 'staging.user_activity_log',
                                     #           'conn_args': pg_conn},
                                    dag=dag)

load_price_log = PythonOperator(task_id='user_order_log',
                                    python_callable=load_file_to_pg,
                                    #op_kwargs={'filename': business_dt.replace('-','') + '_price_log.csv',
                                     #           'pg_table': 'staging.price_log',
                                      #          'conn_args': pg_conn},
                                    dag=dag)

update_dimensions = PythonOperator(task_id='update_dimensions',
                                    python_callable=pg_execute_query,
                                    #op_kwargs={'query': dim_upd_sql_query,
                                    #            'conn_args': pg_conn},
                                    dag=dag)

update_facts = PythonOperator(task_id='update_facts',
                                    python_callable=pg_execute_query,
                                    #op_kwargs={'query': facts_upd_sql_query,
                                     #           'conn_args': pg_conn},
                                    dag=dag)
delay_task = BashOperator(task_id="delay_task",
                          bash_command='sleep 5m',
                          dag=dag)


create_files_request >> get_files >> [load_customer_research, load_user_order_log, load_user_activity_log, load_price_log] >> update_dimensions >> update_facts >> delay_task
